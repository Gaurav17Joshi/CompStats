{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Information Theory\n",
    "\n",
    "In layman terms Data Analysis is the process of extracting useful information from data. But what is information? How do we measure it?\n",
    "\n",
    "In this notebook, we will explore the concept of information, uncertainity in Random Variables and how it can be quantified. We will also be looking at a way to quantify the magnitude of an update from one set of beliefs to another using the concept of KL Divergence. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entropy\n",
    "\n",
    "> Entropy is the average amount of \"information\", \"surprise\", or \"uncertainty\" inherent in the variable's possible outcomes.\n",
    "\n",
    "For a random variable $X$ with $n$ possible outcomes, the entropy $H(X)$ is defined as: \n",
    "\n",
    "$$H(X) = -\\sum_{i=1}^{n} p(x_i) \\log p(x_i)$$\n",
    "\n",
    "where $p(x_i)$ is the probability of the $i^{th}$ outcome.\n",
    "\n",
    "Here we share a simple intution of entropy. Suppose we have two events x and y, then the info gained on observing both would be sum of info gained on observing x and y individually $h(x,y) = h(x)+h(y)$. Also, the probability will have the relation $p(x,y) = p(x) p(y)$. From this we can infer that $h(x)$ must be given by a logarithm of $p(x)$.\n",
    "\n",
    "$$\n",
    "h(x) = -\\log p(x)\n",
    "$$\n",
    "\n",
    "Now, for a random variable $X$ with a state $x_1$, the info we gain on it occuring is $h(x_1) = -\\log p(x_1)$, but it also only occurs with probability $p(x_1)$, so the average info we gain is $h(X) = -\\sum_{i=1}^{n} p(x_i) \\log p(x_i)$.\n",
    "\n",
    "\n",
    "The base of the logarithm is arbitrary. If we use base 2, then the unit of information is the bit. If we use base e, then the unit of information is the nat. If we use base 10, then the unit of information is the digit.\n",
    "\n",
    "**Entropy as Length of Transmitted Code**\n",
    "\n",
    "If we have 8 states (a,b,c,d,e,f,g, h) with probabilities ($\\frac{1}{2}, \\frac{1}{4}, \\frac{1}{8}, \\frac{1}{16}, \\frac{1}{64}, \\frac{1}{64}, \\frac{1}{64}, \\frac{1}{64}$) respectively, then the entropy is: \n",
    "\n",
    "$$\n",
    "H(X) = -\\frac{1}{2} \\log \\frac{1}{2} - \\frac{1}{4} \\log \\frac{1}{4} - \\frac{1}{8} \\log \\frac{1}{8} - \\frac{1}{16} \\log \\frac{1}{16} - \\frac{4}{64} \\log \\frac{1}{64} = -2\n",
    "$$\n",
    "\n",
    "This means that on average, we need 2 bits to encode the state of the random variable.\n",
    "We can represent the codes as (a: 0, b: 10, c: 110, d: 1110, e: 111100, f: 111101, g: 111110, h: 111111). The pdf average length of the code is 2 bits.\n",
    "\n",
    "**Differential Entropy**\n",
    "\n",
    "The continuous version of entropy is called differential entropy. It is defined as:\n",
    "\n",
    "$$\n",
    "H(X) = -\\int_{-\\infty}^{\\infty} p(x) \\log p(x) dx\n",
    "$$\n",
    "\n",
    "Note, this can also be in negative as pdf can be greater than 1.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joint Entropy\n",
    "\n",
    "Joint entropy is the extension of entropy to multiple random variables. For two random variables $X$ and $Y$, the joint entropy $H(X,Y)$ is defined as:\n",
    "\n",
    "$$H(X,Y) = -\\sum_{i=1}^{n} \\sum_{j=1}^{m} p(x_i,y_j) \\log p(x_i,y_j)$$\n",
    "\n",
    "where $p(x_i,y_j)$ is the joint probability of the $i^{th}$ outcome of $X$ and the $j^{th}$ outcome of $Y$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Entropy\n",
    "\n",
    "> Cross-entropy is a measure of the difference between two probability distributions for a given random variable or set of events. **Note**: on same set.\n",
    "\n",
    "Till now, we have been looking at the entropy of a single random variable. But what if we have two random variables $X$ and $Y$ with the same set of outcomes? In this case, we can use the cross entropy to get a measure of the difference between the two probability distributions over the same underlying set of events.\n",
    "\n",
    "$$H_{ce} = -\\sum_{i=1}^{n} p(x_i) \\log q(x_i)$$\n",
    "\n",
    "The intution between why it gives a measure of the difference between the two probability distributions will become clearer after we look at KL Divergence. \n",
    "\n",
    "It shows up in the Cross entropy loss function in logistic learning, where we are trying to minimize the difference between the actual output (a one hot vector, which acts like a probibility distribution), and the predicted distribution of probabilites for different classes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mutual Information\n",
    "\n",
    "> Mutual information is a measure of the amount of information that one random variable contains about another random variable.\n",
    "\n",
    "For two random variables $X$ and $Y$, the mutual information $I(X;Y)$ is defined as:\n",
    "\n",
    "$$I(X;Y) = \\sum_{i=1}^{n} \\sum_{j=1}^{m} p(x_i,y_j) \\log \\frac{p(x_i,y_j)}{p(x_i)p(y_j)}$$\n",
    "\n",
    "It intuitively measures how much information is shared between the two random variables. If $X$ and $Y$ are independent, then $I(X;Y) = 0$, and it increases the more dependent they become. \n",
    "\n",
    "Here we can recognise that $\\frac{p(x_i,y_j)}{p(x_i)p(y_j)}$, is the correlation between the two variables, and the mutual information is the expectation of the log of the correlation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KL Divergence\n",
    "\n",
    "> KL Divergence is a measure of how one probability distribution is different from a second, reference probability distribution.\n",
    "\n",
    "Information in itself is a vague and abstract concept. But we can quantify the magnitude of an update from one set of beliefs to another very well using the concept of KL Divergence. Here we will show the KL divergence formulae, understand some of its properties and relate them to entropy and cross entropy.\n",
    "\n",
    "For two probability distributions $p(x)$ and $q(x)$, the KL divergence $D_{KL}(p||q)$ is defined as:\n",
    "\n",
    "$$D_{KL}(p||q) = \\sum_{i=1}^{n} p(x_i) \\log \\frac{p(x_i)}{q(x_i)}$$\n",
    "\n",
    "In the continuous case, it is defined as:\n",
    "\n",
    "$$D_{KL}(p||q) = \\int_{-\\infty}^{\\infty} p(x) \\log \\frac{p(x)}{q(x)} dx$$\n",
    "\n",
    "This quantifies the information update on changing from prior belief $q(x)$ to posterior belief $p(x)$. \n",
    "\n",
    "### KLD properties\n",
    "\n",
    "**1. Continuity of KL Divergence**\n",
    "    \n",
    "> KL Divergence is continuous in the limit as $p \\to 0$ and $q \\to 0$.\n",
    "\n",
    "The information gain should be continous. The KL divergence by its formulae is continous, but we have to check the cases where $p(x) = 0$ and $q(x) = 0$. \n",
    "\n",
    "In the limit $p \\to 0$, we have $ \\lim_{p \\to 0} p \\log \\frac{p}{q} \\to 0$, so the KL divergence is continous at $p = 0$.\n",
    "\n",
    "For $q \\to 0$, we have $ \\lim_{q \\to 0} p \\log \\frac{p}{q} \\to \\infty$, but, this is expected as if $q$ (prior) is zero at some point, then if the posterior $p$ is non-zero at that point, then the information gain is infinite, as we had not expected the event to occur at all. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Non Negativity of KL Divergence**\n",
    "\n",
    "> KL Divergence is non-negative, and is equal to zero if and only if $p(x) = q(x)$.\n",
    "\n",
    "The information gain should be positive regardless of the probability distributions, as we always gain information on changing our beliefs.\n",
    "\n",
    "We will make use of the **Jensens Inequality**, which states that for a convex function $f(x)$, we have:\n",
    "\n",
    "$$f(\\sum_{i} \\lambda_i x_i) \\leq \\sum_{i} \\lambda_i f(x_i)$$\n",
    "\n",
    "```{figure} ../images/Maths_appendix/Jensen.png\n",
    "--- \n",
    "height: 300px\n",
    "width: 700px\n",
    "name: Jensen Inequality\n",
    "---\n",
    "\n",
    "Jensen Inequality\n",
    "```\n",
    "\n",
    "Jensens Inequality is the equation for the statement that in a convex function, all secant lines lie above the graph of the function. This is also evident by the figure above.\n",
    "\n",
    "**Proof:** $D_{KL}(p||q) \\geq 0$ with equality if and only if $p(x) = q(x)$\n",
    "\n",
    "\\begin{align}\n",
    "-D_{KL}(p||q) &= -\\sum_{x} p(x) \\log \\frac{p(x)}{q(x)} = \\sum_{x} p(x) \\log \\frac{q(x)}{p(x)}  \\\\\n",
    "& \\leq \\log \\sum_{x} p(x) \\frac{q(x)}{p(x)} = \\log \\sum_{x} q(x) = 0 \\\\\n",
    "& \\leq \\log \\sum_{x} q(x) = log(1) = 0\n",
    "\\end{align}\n",
    "\n",
    "The equality holds when $p(x) = q(x)$. Note: We used the fact that log is a concave function, so we can apply inverse Jensens Inequality.\n",
    "\n",
    "The non-negativity of the KL divergence is a very important property, as it allows us to get bounds on expressions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Chain Rule for KL Divergence**\n",
    "\n",
    "> KL Diveregence is additive, i.e. $D_{KL}(p(x,y)||q(x,y)) = D_{KL}(p(x)||q(x)) + D_{KL}(p(y|x)||q(y|x))$\n",
    "\n",
    "For probabilities we have the chain rule (product rule) as:\n",
    "\n",
    "$$p(x,y) = p(x|y)p(y) = p(y|x)p(x)$$\n",
    "\n",
    "Is there a similar rule for KL divergence? Can we split the information gain from two variables in a chain rule as we did for probabilities?\n",
    "\n",
    "\\begin{align}\n",
    "D_{KL}(p(x,y)||q(x,y)) &= \\int_{x} \\int_{y} dx \\; dy \\; p(x,y) \\log \\frac{p(x,y)}{q(x,y)} \\\\\n",
    "&= \\int_{x} \\int_{y} dx \\; dy \\; p(x,y) \\left[ \\log \\frac{p(x)}{q(x)} + \\log \\frac{p(y|x)}{q(y|x)} \\right] \\\\\n",
    "&= \\int_{x} dx \\; \\log \\frac{p(x)}{q(x)} \\int_{y}  p(x,y) \\; dy + \\int_{x} p(x) \\; dx \\int_{y} p(y|x) \\log \\frac{p(y|x)}{q(y|x)} \\; dy \\\\\n",
    "& = \\int_{x} dx \\; p(x) \\log \\frac{p(x)}{q(x)} +  \\int_{x} p(x) D_{KL}(p(y|x)||q(y|x)) \\; dx  \\\\ \n",
    "& = D_{KL}(p(x)||q(x)) + D_{KL}(p(y|x)||q(y|x))\n",
    "\\end{align}\n",
    "\n",
    "Here we have put $D_{KL}(p(y|x)||q(y|x))$ inplace of $\\mathbb{E}_{p(x)}[D_{KL}(p(y|x)||q(y|x))]$, as the expectation is taken over $p(x)$, and $p(y|x)$ is a function of $x$, so it is constant with respect to $p(x)$.\n",
    "\n",
    "This means that just like the probabilities, we can also use a chain rule for the KL divergence. This is a very important property, as it allows us to split the information gain from multiple variables into individual information gains.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. KL Divergence is invariant to reparametrisation**\n",
    "\n",
    "> KL Divergence is invariant to reparametrisation of the variables.\n",
    "\n",
    "What happens when we reparametrise our functions from $x \\to y$, The KL Divergence should ideally not change, as we are just changing the way we are representing the same information, and it does not.\n",
    "\n",
    "If we change the variable from $x \\to y$, then we also have $p(x) \\; dx = p(y) \\; dy $\n",
    "\n",
    "\\begin{align}\n",
    "D_{KL}(p(x)||q(x)) &= \\int_{x} p(x) \\log \\frac{p(x)}{q(x)} \\; dx \\\\\n",
    "&= \\int_{y} p(y) \\frac{dy}{dx} \\log \\frac{p(y) |\\frac{dy}{dx}| }{q(y) |\\frac{dy}{dx}|} \\; dx \\\\\n",
    "&= \\int_{y} p(y) \\log \\frac{p(y)}{q(y)} \\; dy \\\\\n",
    "\\end{align}\n",
    "\n",
    "Because of this reparameterization invariance we can rest assured that when we measure the KL divergence between two distributions we are measuring something about the distributions and not the way we choose to represent the space in which they are defined. We are therefore free to transform our data into a convenient basis of our choosing, such as a Fourier bases for images, without affecting the result.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. **KL Divergence is not Symmetric**\n",
    "\n",
    "> KL Divergence is not symmetric, i.e. $D_{KL}(p(x)||q(x)) \\neq D_{KL}(q(x)||p(x))$\n",
    "\n",
    "\n",
    "By looking at its formulae itself, we can say KL Divergence is not symmetric, but should'nt it be? After all the information gained on changing from $p(x)$ to $q(x)$ should be the same as the information gained on changing from $q(x)$ to $p(x)$. But, this seemingly obvious statement is not true. Case in point: The info gained on going from a neutral event to a rare event is lower than that of going from the rare event to the neutral event.\n",
    "\n",
    "```{figure} ../images/Maths_appendix/assymofKL.png\n",
    "--- \n",
    "height: 260px\n",
    "width: 700px\n",
    "name: assymofKL\n",
    "---\n",
    "\n",
    "Assymetry of KLD\n",
    "```\n",
    "\n",
    "In this example, we took two different Beta distributions, Beta1 = Beta($\\alpha = 3, \\beta =3$) and Beta2 = Beta($\\alpha = 16, \\beta = 3$). This clearly shows our point. (Similar analysis can be done with the slider code given below)\n",
    "\n",
    "As another Example:-\n",
    "\n",
    "Suppose we have a biased coin: Initially we were told that it shows heads with a probability 0.443, and then told that no, actually it has a probability 0.975 of showing heads. The information gain is:\n",
    "\n",
    "$$D_{KL}(p||q) = p \\log_{2} \\frac{p}{q} + (1-p) \\log_{2} \\frac{1-p}{1-q} = 1 $$\n",
    "\n",
    "If we flip the game then the information gained will be:-\n",
    "\n",
    "$$D_{KL}(q||p) = q \\log_{2} \\frac{q}{p} + (1-q) \\log_{2} \\frac{1-q}{1-p} = 2 $$\n",
    "\n",
    "Thus we see that starting with a distribution that is nearly even and moving to one that is nearly certain takes about 1 bit of information, or one well designed yes/no question. To instead move us from near certainty in an\n",
    "outcome to something that is akin to the flip of a coin requires more persuasion.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Info gain1:  0.9977011988907731\n",
      "Info gain2:  1.9898899560575691\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "q = 0.443; p = 0.975\n",
    "print(\"Info gain1: \", p*math.log2(p/q) + (1-p)*math.log2((1-p)/(1-q)))\n",
    "\n",
    "p = 0.443; q = 0.975\n",
    "print(\"Info gain2: \", p*math.log2(p/q) + (1-p)*math.log2((1-p)/(1-q)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating the beta distribution klds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e97a82a696034138b41fb8700a3f6895",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=10.0, description='alpha1', max=19.0, min=1.0, step=1.0), FloatSlider(…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_beta(alpha1, beta1, alpha2, beta2)>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import interact\n",
    "\n",
    "def calc_info_gain(y1, y2):\n",
    "    L = 200\n",
    "    return 1/L*torch.sum(y2*torch.log(y2/y1))\n",
    "\n",
    "def plot_beta(alpha1, beta1, alpha2, beta2):\n",
    "    Beta1 = torch.distributions.Beta(concentration1=alpha1, concentration0=beta1)\n",
    "    Beta2 = torch.distributions.Beta(concentration1=alpha2, concentration0=beta2)\n",
    "    xs = torch.linspace(0.01, 0.99, 100)\n",
    "    ys1 = Beta1.log_prob(xs).exp()\n",
    "    plt.plot(xs, ys1, color='C0', label = \"prior Beta\")#label='Beta' + str(alpha1) + ',' + str(beta1))\n",
    "    ys2 = Beta2.log_prob(xs).exp()\n",
    "    plt.plot(xs, ys2, color='C1', label = \"post Beta\")#label = 'Beta' + str(alpha2) + ',' + str(beta2))\n",
    "    plt.legend()\n",
    "    # Filled area\n",
    "    plt.fill_between(xs, ys1, color='C0', alpha=0.2)\n",
    "    plt.fill_between(xs, ys2, color='C1', alpha=0.2)\n",
    "    # write title with info gain only till 5 decimal places\n",
    "    plt.title('Information Gain: ' + str(round(calc_info_gain(ys1, ys2).item(), 5)))\n",
    "\n",
    "interact(plot_beta,alpha1=(1.0, 19, 1.0), beta1=(1.0, 19, 1.0), alpha2=(1.0, 19, 1.0), beta2=(1.0, 19, 1.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{figure} ../images/Maths_appendix/Sliderexample.png\n",
    "--- \n",
    "height: 500px\n",
    "width: 600px\n",
    "name: Slider Example\n",
    "---\n",
    "\n",
    "Slider Example\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entropy and KLD\n",
    "\n",
    "> Entropy of a distribution p is a contant minus (-) the KL Divergence of that distribution from the uniform distribution.\n",
    "\n",
    "The entropy of a distribution tries to capture the information, uncertainity of the pdf and the KL Divergence gives us the information gained on updating our belief of a pdf. So, it is natural to ask if there is a relation between the two. \n",
    "\n",
    "For Discrete Case:\n",
    "\n",
    "$$\n",
    "KLD = \\sum_{i=1}^{n} p(x_i) \\log \\frac{p(x_i)}{q(x_i)} \\; , \\; Entropy \\;  = -\\sum_{i=1}^{n} p(x_i) \\log p(x_i)\n",
    "$$\n",
    "\n",
    "We can use $q_i = \\frac{1}{n}$ as the uniform distribution, and we get:\n",
    "\n",
    "\\begin{align}\n",
    "KLD &= \\sum_{i=1}^{n} p(x_i) \\log \\frac{p(x_i)}{q(x_i)} \\\\\n",
    "&= \\sum_{i=1}^{n} p(x_i) \\log \\frac{p(x_i)}{\\frac{1}{n}} \\\\\n",
    "&= \\sum_{i=1}^{n} p(x_i) \\log p(x_i) + \\sum_{i=1}^{n} p(x_i) \\log n \\\\\n",
    "&=  \\log n - Entropy\n",
    "\\end{align}\n",
    "\n",
    "Hense,\n",
    "\n",
    "$$\n",
    "Entropy = constant - KLD\n",
    "$$\n",
    "\n",
    "This also feels intuitive as the uniform distribution (U) is the most uncertain distribution, so it should have the maximum entropy and it does as it has the least KLD with U (ie 0).\n",
    "\n",
    "It also makes sense in another way, as for any other distribution p, the more its KLD with U means more information gained on updating our belief from U to p, which means its prob is more peaked (certain), hense its lesser entropy, as shown by the formulae.\n",
    "\n",
    "\n",
    "For Continuous Case:\n",
    "\n",
    "Entropy is similar in the continuous case, where it is called continuous entropy, but with one change. Discrete entropy is always positive, but continuous entropy can be negative as pdf can be greater than 1. (Even U has Entropy 0, as log(1) = 0)\n",
    "\n",
    "$$\n",
    "KLD = \\int_{}^{} p(x) \\log \\frac{p(x)}{q(x)} dx \\; , \\; Entropy \\;  = -\\int_{}^{} p(x) \\log p(x) dx\n",
    "$$\n",
    "\n",
    "\n",
    "You can see from the slider example that the entropy of a distribution is a constant minus the KL Divergence of that distribution from the uniform distribution. (Set the prior as a beta dist of $\\alpha = 1, \\beta = 1$)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Entropy and KLD\n",
    "\n",
    "The cross entropy was our previous way to get some measure of closeness of two probability distributions and it is related to the KL Divergence.\n",
    "\n",
    "\\begin{align}\n",
    "KLD &= \\sum_{i=1}^{n} p(x_i) \\log \\frac{p(x_i)}{q(x_i)} \\\\\n",
    "&= \\sum_{i=1}^{n} p(x_i) \\log p(x_i) - \\sum_{i=1}^{n} p(x_i) \\log q(x_i) \\\\\n",
    "&= -H(p) + H_{ce}(p,q)\n",
    "\\end{align}\n",
    "\n",
    "Where $H_{p}$ is the entropy of the distribution $p$ and $H_{ce}$ is the cross entropy of the distributions $p$ and $q$.\n",
    "\n",
    "$$\n",
    "H_{ce}(p,q) = -\\sum_{i=1}^{n} p(x_i) \\log q(x_i)\n",
    "$$\n",
    "\n",
    "One way to think:-\n",
    "\n",
    "In KLDivergence, we are trying to get the information gained on updating our belief from $q$ to $p$. Cross entropy gives us the uncertainity when we are trying to update our belief from $q$ to $p$ but using $q$ to encode the events. Entropy of $p$ gives us the uncertainity inherent in $p$. So on adding the KLDivergence and the entropy we get the cross entropy.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MI and KLD\n",
    "\n",
    "> MI measures the information gain if we update from a model that treats the two variables as independent p(x)p(y) to one that models their true joint density p(x, y).\n",
    "\n",
    "KLDivergence and MI are obviously linked as KLD tells us how much info we gain on going from Y tp X, while MI gives us how much info of Y was being kept in X.\n",
    "\n",
    "$$\n",
    "I(X;Y) = D_{KL} (p(x,y)||p(x)p(y)) = \\sum_{i=1}^{n} \\sum_{j=1}^{m} p(x_i,y_j) \\log \\frac{p(x_i,y_j)}{p(x_i)p(y_j)}\n",
    "$$\n",
    "\n",
    "Now, by formula, MI is measuring the information gained on updating our belief from $p(x)p(y)$ to $p(x,y)$. The divergence is zero if $p(x,y) = p(x)p(y)$, which means the two distributions are independent, which should give us a Mutual information 0 (which it does). \n",
    "\n",
    "So the more $p(x,y)$ is different from $p(x)p(y)$, the more information they keep of each other, and the more MI they have.\n",
    "\n",
    "Another interpretation:-\n",
    "\n",
    "$$\n",
    "I(X;Y) = H(X) - H(X|Y) = H(Y) - H(Y|X)\n",
    "$$\n",
    "\n",
    "So, MI is quantifies how uncertain we are about X - how uncertain we are about X given that we know Y, or it is the difference between the entropy of the random variable and the conditional entropy of the random variable given the other random variable. This means that MI is the information that the two random variables share with each other.\n",
    "\n",
    "Thus we can interpret the MI between X and Y as the reduction in uncertainty about X after observing Y , or, by symmetry, the reduction in uncertainty about Y after observing X. Incidentally, this result gives an alternative proof that conditioning, on average, reduces entropy. In particular, we have $ 0 \\leq I(X;Y) = H(X) − H(X|Y)$ ,and hence $H(X|Y) \\leq H(X)$.\n",
    "\n",
    "We can also show that\n",
    "\n",
    "$$\n",
    "I(X;Y) = H(X) + H(Y) - H(X,Y)\n",
    "$$\n",
    "\n",
    "These relations are captured perfectly by this figure from Kevin Murphy\n",
    "\n",
    "```{figure} ../images/Maths_appendix/Entropies.png\n",
    "--- \n",
    "height: 400px\n",
    "width: 500px\n",
    "name: Entropies\n",
    "---\n",
    "\n",
    "Entropies\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Information Of Data**\n",
    "\n",
    "How to charachterise the info of 111000 vs 100101 vs 101010?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "statsenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}