{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latent Variable Models\n",
    "\n",
    "Latent Variable models is a general term for models which contain latent variables. These are hidden and not observed, but we may either be interested in them or may have to marginalise them out to perform inference. They can be used for\n",
    "expressing complicated densities that cannot be described by an exponential family distribution, or to cluster data points.\n",
    "\n",
    "The simplest of such models is the Gaussian Mixture Model (GMM), which we will investigate.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian Mixture Model\n",
    "\n",
    "```{figure} ../images/Prob_Modeling/GMMs.png\n",
    "---\n",
    "height: 200px\n",
    "width: 400px\n",
    "name: GMMs\n",
    "---\n",
    "\n",
    "GMM Graphical Representation, Synthetic Data\n",
    "```\n",
    "\n",
    "In this model, we assume that the data is generated from a mixture of Gaussians, where the latent variable is the identity of the Gaussian from which the data was generated. The model is as follows:\n",
    "\n",
    "* $z_n$ is the multinomial latent variable, which determines the mean $x_n$ is centered around. \n",
    "\n",
    "* $\\pi$ is a parameter for  $z_n$, which is a vector of probabilities for each of the $K$ Gaussians. $\\pi = (\\pi_1, ...\\pi_k)$, where $\\pi_i$ is the probability that $z_n = i$.\n",
    "\n",
    "* $\\mu$ is also a vector of length $K$, where $\\mu_i$ is the mean of the $i$th Gaussian.\n",
    "\n",
    "The joint can be written as follows:\n",
    "\n",
    "$$\n",
    "p(x_{1:N}, z_{1:N} | \\pi, \\mu_{1:K}) = \\prod_{n=1}^N p(z_n | \\pi) p(x_n | z_n, \\mu_{1:K})\n",
    "$$\n",
    "\n",
    "In our interest to do ML estimation on the parameters of the model, we\n",
    "need to obtain the marginal probability of the data given the parameters. To\n",
    "do so it is necessary to marginalize out unobserved variables, which in our\n",
    "case are the cluster assignments:\n",
    "\n",
    "$$\n",
    "p(x_{1:N} | \\pi, \\mu_{1:K}) = \\prod_{n=1}^N \\sum_{z} p(z_n | \\pi) p(x_n | z_n, \\mu_{1:K})\n",
    "$$\n",
    "\n",
    "The log likelihood is then:\n",
    "\n",
    "\\begin{align}\n",
    "\\log p(x_{1:N} | \\pi, \\mu_{1:K}) &= \\sum_{n=1}^N \\log \\sum_{z} p(z_n | \\pi) p(x_n | z_n, \\mu_{1:K}) \\\\\n",
    "&= \\sum_{n=1}^N \\log \\biggl[ \\sum_{z}  \\biggl( \\prod_{i} \\mu_i^{z_n^i} \\biggr) \\biggl( \\prod_{i} p(x_n | \\mu_i)^{z_n^i} \\biggr)  \\biggr] \\\\\n",
    "\\end{align}\n",
    "\n",
    "Because we had to marginalize out latent variables the sum is inside the logarithm, which makes the maximization problem cumbersome.\n",
    "\n",
    "We could simply use a black-box algorithm for optimization. However, we can exploit characteristics of the log likelihood, and we do so by deriving\n",
    "the EM (expectation-maximization) algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
